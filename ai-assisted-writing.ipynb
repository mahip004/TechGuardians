{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\itsme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\itsme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\itsme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    relevant_words = []\n",
    "    for word, pos in pos_tags:\n",
    "        if(pos.startswith(('VB', 'JJ', 'RB'))):\n",
    "            relevant_words.append(word)\n",
    "    return relevant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(word1, word2):\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "    max_similarity = 0\n",
    "    \n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.path_similarity(synset2)\n",
    "            if similarity and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                \n",
    "    return max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_synonyms(text):\n",
    "    relevant_words = find_relevant_words(text)\n",
    "    dict = {}\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for word, pos in pos_tags:\n",
    "        if word in relevant_words:\n",
    "            synonyms = get_synonyms(word)\n",
    "            text = []\n",
    "            count = 0\n",
    "            for synonym in synonyms:\n",
    "                if(synonym!=word):\n",
    "                    similarity = word_similarity(word, synonym)\n",
    "                    if similarity and similarity >= 0.99 and count<=4:\n",
    "                        count = count+1\n",
    "                        text.append(synonym)\n",
    "            dict[word] = text       \n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(text, synonym_dict, max_sentences=7):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    generate_sentences_set = set()\n",
    "    word_options = []\n",
    "    for word in tokens:\n",
    "        if word in synonym_dict:\n",
    "            word_options.append([word] + synonym_dict[word])\n",
    "        else:\n",
    "            word_options.append([word])\n",
    "    \n",
    "    all_combinations = list(itertools.product(*word_options))\n",
    "    \n",
    "    for combination in all_combinations:\n",
    "        sentence = ' '.join(combination)\n",
    "        if(sentence!=text):\n",
    "            generate_sentences_set.add(sentence)\n",
    "        if len(generate_sentences_set) >= max_sentences:\n",
    "            break\n",
    "    \n",
    "    return generate_sentences_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: The future boom that was predicted, now seems uncertain\n",
      "Generated sentence 1: The future boom that was predicted , now seems incertain\n",
      "Generated sentence 2: The future boom that was predicted , now seems changeable\n",
      "Generated sentence 3: The future boom that was predicted , now seems uncertain\n",
      "\n",
      "\n",
      "Original sentence: The expected results will not be met\n",
      "Generated sentence 1: The expected results will not be fill\n",
      "Generated sentence 2: The expected results will not be take_on\n",
      "Generated sentence 3: The expected results will not be fulfil\n",
      "\n",
      "\n",
      "Original sentence: Sales are expected to drop by half by the end of the year\n",
      "Generated sentence 1: Sales are expected to sink by half by the end of the year\n",
      "Generated sentence 2: Sales are expected to drip by half by the end of the year\n",
      "Generated sentence 3: Sales are expected to drop_off by half by the end of the year\n",
      "\n",
      "\n",
      "Original sentence: Customer 365 days expiry date starts from the date of sim activation\n",
      "Generated sentence 1: Customer 365 days decease date starts from the date of sim activation\n",
      "Generated sentence 2: Customer 365 days death date starts from the date of sim activation\n",
      "Generated sentence 3: Customer 365 days expiration date starts from the date of sim activation\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Customer 365 days expiry date starts from the date of sim activation. Sales are expected to drop by half by the end of the year. The future boom that was predicted, now seems uncertain. The expected results will not be met.\"\n",
    "sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "\n",
    "selected_sentences = random.sample(sentences, min(5, len(sentences)))\n",
    "    \n",
    "for selected_sentence in selected_sentences:\n",
    "    print(f\"Original sentence: {selected_sentence}\")\n",
    "    synonym_dict = replace_with_synonyms(selected_sentence)\n",
    "    unique_sentences = generate_sentences(selected_sentence, synonym_dict, max_sentences=3)\n",
    "    for i, sentence in enumerate(unique_sentences, 1):\n",
    "        print(f\"Generated sentence {i}: {sentence}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
